2023-11-10 02:35:55,267 - root - INFO - 
---Filtering Start---
2023-11-10 02:35:55,267 - root - INFO - Log file is ./DeepSVDD/log/log1699601755.2671099.txt.
2023-11-10 02:35:55,267 - root - INFO - GPU is available.
2023-11-10 02:35:55,270 - root - INFO - Start analyzing normal class: 0 / 7
2023-11-10 02:35:55,273 - root - INFO - Set seed to 42.
2023-11-10 02:35:55,273 - root - INFO - Computation device: cuda
2023-11-10 02:35:55,273 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:35:55,278 - root - INFO - Pretraining: True
2023-11-10 02:35:55,278 - root - INFO - 
---Pretraining Start---
2023-11-10 02:35:55,278 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:35:55,278 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:35:55,278 - root - INFO - Pretraining epochs: 4
2023-11-10 02:35:55,278 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:35:55,278 - root - INFO - Pretraining batch size: 20
2023-11-10 02:35:55,278 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:35:55,348 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:35:56,440 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:35:56,486 - root - INFO - Epoch: 1/4
	  Time:       1.137 sec
	  Train Loss: 56561197.33333334
	  Test Loss:  67791013.33333333
	  Test AUC:   65.91

2023-11-10 02:35:56,618 - root - INFO - Epoch: 2/4
	  Time:       0.131 sec
	  Train Loss: 55537642.66666666
	  Test Loss:  67785512.00000000
	  Test AUC:   65.91

2023-11-10 02:35:56,757 - root - INFO - Epoch: 3/4
	  Time:       0.138 sec
	  Train Loss: 55790217.33333334
	  Test Loss:  67785230.66666667
	  Test AUC:   65.91

2023-11-10 02:35:56,893 - root - INFO - Epoch: 4/4
	  Time:       0.135 sec
	  Train Loss: 55566982.66666666
	  Test Loss:  67786017.33333333
	  Test AUC:   65.91

2023-11-10 02:35:56,893 - root - INFO - Pretraining time: 1.546
2023-11-10 02:35:56,894 - root - INFO - Finished pretraining.
2023-11-10 02:35:56,900 - root - INFO - Testing autoencoder...
2023-11-10 02:35:56,924 - root - INFO - Test set Loss: 67786017.33333333
2023-11-10 02:35:56,924 - root - INFO - Autoencoder testing time: 0.024
2023-11-10 02:35:56,924 - root - INFO - Finished testing autoencoder.
2023-11-10 02:35:56,928 - root - INFO - 
---Training Start---
2023-11-10 02:35:56,929 - root - INFO - Training optimizer: adam
2023-11-10 02:35:56,929 - root - INFO - Training learning rate: 0.001
2023-11-10 02:35:56,929 - root - INFO - Training epochs: 2
2023-11-10 02:35:56,929 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:35:56,929 - root - INFO - Training batch size: 20
2023-11-10 02:35:56,929 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:35:56,932 - root - INFO - Initializing center c...
2023-11-10 02:35:56,939 - root - INFO - Center c initialized.
2023-11-10 02:35:56,939 - root - INFO - Starting training...
2023-11-10 02:35:56,984 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:35:56,984 - root - INFO - Epoch: 1/2
	  Time:       0.045 sec
	  Train Loss: 45210.99088542
	  Test AUC:   40.34

2023-11-10 02:35:57,022 - root - INFO - Epoch: 2/2
	  Time:       0.037 sec
	  Train Loss: 44458.60026042
	  Test AUC:   40.34

2023-11-10 02:35:57,022 - root - INFO - Training time: 0.083
2023-11-10 02:35:57,022 - root - INFO - Finished training.
2023-11-10 02:35:57,395 - root - INFO - Start analyzing normal class: 1 / 7
2023-11-10 02:35:57,398 - root - INFO - Set seed to 42.
2023-11-10 02:35:57,398 - root - INFO - Computation device: cuda
2023-11-10 02:35:57,398 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:35:57,402 - root - INFO - Pretraining: True
2023-11-10 02:35:57,402 - root - INFO - 
---Pretraining Start---
2023-11-10 02:35:57,402 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:35:57,402 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:35:57,402 - root - INFO - Pretraining epochs: 4
2023-11-10 02:35:57,402 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:35:57,402 - root - INFO - Pretraining batch size: 20
2023-11-10 02:35:57,402 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:35:57,455 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:35:57,603 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:35:57,633 - root - INFO - Epoch: 1/4
	  Time:       0.177 sec
	  Train Loss: 73306453.33333333
	  Test Loss:  67787977.33333333
	  Test AUC:   64.19

2023-11-10 02:35:57,770 - root - INFO - Epoch: 2/4
	  Time:       0.136 sec
	  Train Loss: 72063221.33333333
	  Test Loss:  67783194.66666667
	  Test AUC:   64.19

2023-11-10 02:35:57,911 - root - INFO - Epoch: 3/4
	  Time:       0.140 sec
	  Train Loss: 72064906.66666667
	  Test Loss:  67784509.33333333
	  Test AUC:   64.19

2023-11-10 02:35:58,050 - root - INFO - Epoch: 4/4
	  Time:       0.137 sec
	  Train Loss: 72272592.00000000
	  Test Loss:  67786466.66666667
	  Test AUC:   64.19

2023-11-10 02:35:58,050 - root - INFO - Pretraining time: 0.594
2023-11-10 02:35:58,050 - root - INFO - Finished pretraining.
2023-11-10 02:35:58,056 - root - INFO - Testing autoencoder...
2023-11-10 02:35:58,080 - root - INFO - Test set Loss: 67786466.66666667
2023-11-10 02:35:58,081 - root - INFO - Autoencoder testing time: 0.024
2023-11-10 02:35:58,081 - root - INFO - Finished testing autoencoder.
2023-11-10 02:35:58,085 - root - INFO - 
---Training Start---
2023-11-10 02:35:58,085 - root - INFO - Training optimizer: adam
2023-11-10 02:35:58,085 - root - INFO - Training learning rate: 0.001
2023-11-10 02:35:58,085 - root - INFO - Training epochs: 2
2023-11-10 02:35:58,085 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:35:58,085 - root - INFO - Training batch size: 20
2023-11-10 02:35:58,085 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:35:58,089 - root - INFO - Initializing center c...
2023-11-10 02:35:58,096 - root - INFO - Center c initialized.
2023-11-10 02:35:58,096 - root - INFO - Starting training...
2023-11-10 02:35:58,134 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:35:58,134 - root - INFO - Epoch: 1/2
	  Time:       0.038 sec
	  Train Loss: 58310.86848958
	  Test AUC:   36.74

2023-11-10 02:35:58,171 - root - INFO - Epoch: 2/2
	  Time:       0.037 sec
	  Train Loss: 56947.23567708
	  Test AUC:   35.35

2023-11-10 02:35:58,171 - root - INFO - Training time: 0.075
2023-11-10 02:35:58,171 - root - INFO - Finished training.
2023-11-10 02:35:58,441 - root - INFO - Start analyzing normal class: 2 / 7
2023-11-10 02:35:58,443 - root - INFO - Set seed to 42.
2023-11-10 02:35:58,444 - root - INFO - Computation device: cuda
2023-11-10 02:35:58,444 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:35:58,448 - root - INFO - Pretraining: True
2023-11-10 02:35:58,448 - root - INFO - 
---Pretraining Start---
2023-11-10 02:35:58,448 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:35:58,448 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:35:58,448 - root - INFO - Pretraining epochs: 4
2023-11-10 02:35:58,448 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:35:58,448 - root - INFO - Pretraining batch size: 20
2023-11-10 02:35:58,448 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:35:58,500 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:35:58,650 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:35:58,682 - root - INFO - Epoch: 1/4
	  Time:       0.181 sec
	  Train Loss: 60990010.66666666
	  Test Loss:  67790202.66666667
	  Test AUC:   59.58

2023-11-10 02:35:58,820 - root - INFO - Epoch: 2/4
	  Time:       0.137 sec
	  Train Loss: 63773510.66666666
	  Test Loss:  67783298.66666667
	  Test AUC:   59.58

2023-11-10 02:35:58,969 - root - INFO - Epoch: 3/4
	  Time:       0.148 sec
	  Train Loss: 64325764.00000000
	  Test Loss:  67783821.33333333
	  Test AUC:   59.58

2023-11-10 02:35:59,111 - root - INFO - Epoch: 4/4
	  Time:       0.141 sec
	  Train Loss: 61936305.33333334
	  Test Loss:  67785312.00000000
	  Test AUC:   59.58

2023-11-10 02:35:59,111 - root - INFO - Pretraining time: 0.611
2023-11-10 02:35:59,111 - root - INFO - Finished pretraining.
2023-11-10 02:35:59,118 - root - INFO - Testing autoencoder...
2023-11-10 02:35:59,142 - root - INFO - Test set Loss: 67785312.00000000
2023-11-10 02:35:59,142 - root - INFO - Autoencoder testing time: 0.025
2023-11-10 02:35:59,142 - root - INFO - Finished testing autoencoder.
2023-11-10 02:35:59,147 - root - INFO - 
---Training Start---
2023-11-10 02:35:59,147 - root - INFO - Training optimizer: adam
2023-11-10 02:35:59,147 - root - INFO - Training learning rate: 0.001
2023-11-10 02:35:59,147 - root - INFO - Training epochs: 2
2023-11-10 02:35:59,147 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:35:59,147 - root - INFO - Training batch size: 20
2023-11-10 02:35:59,147 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:35:59,151 - root - INFO - Initializing center c...
2023-11-10 02:35:59,158 - root - INFO - Center c initialized.
2023-11-10 02:35:59,158 - root - INFO - Starting training...
2023-11-10 02:35:59,194 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:35:59,194 - root - INFO - Epoch: 1/2
	  Time:       0.036 sec
	  Train Loss: 52334.38020833
	  Test AUC:   37.28

2023-11-10 02:35:59,232 - root - INFO - Epoch: 2/2
	  Time:       0.038 sec
	  Train Loss: 52159.35546875
	  Test AUC:   40.42

2023-11-10 02:35:59,233 - root - INFO - Training time: 0.074
2023-11-10 02:35:59,233 - root - INFO - Finished training.
2023-11-10 02:35:59,510 - root - INFO - Start analyzing normal class: 3 / 7
2023-11-10 02:35:59,512 - root - INFO - Set seed to 42.
2023-11-10 02:35:59,512 - root - INFO - Computation device: cuda
2023-11-10 02:35:59,512 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:35:59,516 - root - INFO - Pretraining: True
2023-11-10 02:35:59,516 - root - INFO - 
---Pretraining Start---
2023-11-10 02:35:59,516 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:35:59,516 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:35:59,516 - root - INFO - Pretraining epochs: 4
2023-11-10 02:35:59,516 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:35:59,516 - root - INFO - Pretraining batch size: 20
2023-11-10 02:35:59,516 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:35:59,570 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:35:59,682 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:35:59,714 - root - INFO - Epoch: 1/4
	  Time:       0.143 sec
	  Train Loss: 55303178.66666666
	  Test Loss:  67788624.00000000
	  Test AUC:   45.45

2023-11-10 02:35:59,857 - root - INFO - Epoch: 2/4
	  Time:       0.142 sec
	  Train Loss: 56006088.00000000
	  Test Loss:  67782802.66666667
	  Test AUC:   45.45

2023-11-10 02:36:00,000 - root - INFO - Epoch: 3/4
	  Time:       0.142 sec
	  Train Loss: 55178381.33333334
	  Test Loss:  67783440.00000000
	  Test AUC:   45.45

2023-11-10 02:36:00,144 - root - INFO - Epoch: 4/4
	  Time:       0.143 sec
	  Train Loss: 55030120.00000000
	  Test Loss:  67785865.33333333
	  Test AUC:   45.45

2023-11-10 02:36:00,144 - root - INFO - Pretraining time: 0.575
2023-11-10 02:36:00,144 - root - INFO - Finished pretraining.
2023-11-10 02:36:00,151 - root - INFO - Testing autoencoder...
2023-11-10 02:36:00,176 - root - INFO - Test set Loss: 67785865.33333333
2023-11-10 02:36:00,176 - root - INFO - Autoencoder testing time: 0.025
2023-11-10 02:36:00,176 - root - INFO - Finished testing autoencoder.
2023-11-10 02:36:00,181 - root - INFO - 
---Training Start---
2023-11-10 02:36:00,181 - root - INFO - Training optimizer: adam
2023-11-10 02:36:00,181 - root - INFO - Training learning rate: 0.001
2023-11-10 02:36:00,181 - root - INFO - Training epochs: 2
2023-11-10 02:36:00,181 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:36:00,181 - root - INFO - Training batch size: 20
2023-11-10 02:36:00,181 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:36:00,184 - root - INFO - Initializing center c...
2023-11-10 02:36:00,191 - root - INFO - Center c initialized.
2023-11-10 02:36:00,191 - root - INFO - Starting training...
2023-11-10 02:36:00,229 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:00,229 - root - INFO - Epoch: 1/2
	  Time:       0.038 sec
	  Train Loss: 44782.11328125
	  Test AUC:   47.16

2023-11-10 02:36:00,268 - root - INFO - Epoch: 2/2
	  Time:       0.039 sec
	  Train Loss: 43596.07552083
	  Test AUC:   56.82

2023-11-10 02:36:00,268 - root - INFO - Training time: 0.077
2023-11-10 02:36:00,268 - root - INFO - Finished training.
2023-11-10 02:36:00,538 - root - INFO - Start analyzing normal class: 4 / 7
2023-11-10 02:36:00,541 - root - INFO - Set seed to 42.
2023-11-10 02:36:00,541 - root - INFO - Computation device: cuda
2023-11-10 02:36:00,541 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:36:00,611 - root - INFO - Pretraining: True
2023-11-10 02:36:00,612 - root - INFO - 
---Pretraining Start---
2023-11-10 02:36:00,612 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:36:00,612 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:36:00,612 - root - INFO - Pretraining epochs: 4
2023-11-10 02:36:00,612 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:36:00,612 - root - INFO - Pretraining batch size: 20
2023-11-10 02:36:00,612 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:36:00,662 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:36:00,924 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:00,957 - root - INFO - Epoch: 1/4
	  Time:       0.294 sec
	  Train Loss: 83863452.00000000
	  Test Loss:  67784814.66666667
	  Test AUC:   29.29

2023-11-10 02:36:01,213 - root - INFO - Epoch: 2/4
	  Time:       0.255 sec
	  Train Loss: 83903196.00000000
	  Test Loss:  67788193.33333333
	  Test AUC:   29.29

2023-11-10 02:36:01,461 - root - INFO - Epoch: 3/4
	  Time:       0.246 sec
	  Train Loss: 83750921.33333333
	  Test Loss:  67792109.33333333
	  Test AUC:   29.29

2023-11-10 02:36:01,710 - root - INFO - Epoch: 4/4
	  Time:       0.248 sec
	  Train Loss: 84133834.66666667
	  Test Loss:  67794868.00000000
	  Test AUC:   29.29

2023-11-10 02:36:01,710 - root - INFO - Pretraining time: 1.048
2023-11-10 02:36:01,710 - root - INFO - Finished pretraining.
2023-11-10 02:36:01,717 - root - INFO - Testing autoencoder...
2023-11-10 02:36:01,743 - root - INFO - Test set Loss: 67794868.00000000
2023-11-10 02:36:01,743 - root - INFO - Autoencoder testing time: 0.025
2023-11-10 02:36:01,743 - root - INFO - Finished testing autoencoder.
2023-11-10 02:36:01,747 - root - INFO - 
---Training Start---
2023-11-10 02:36:01,747 - root - INFO - Training optimizer: adam
2023-11-10 02:36:01,747 - root - INFO - Training learning rate: 0.001
2023-11-10 02:36:01,747 - root - INFO - Training epochs: 2
2023-11-10 02:36:01,747 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:36:01,747 - root - INFO - Training batch size: 20
2023-11-10 02:36:01,747 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:36:01,750 - root - INFO - Initializing center c...
2023-11-10 02:36:01,763 - root - INFO - Center c initialized.
2023-11-10 02:36:01,763 - root - INFO - Starting training...
2023-11-10 02:36:01,823 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:01,823 - root - INFO - Epoch: 1/2
	  Time:       0.059 sec
	  Train Loss: 67111.69140625
	  Test AUC:   69.49

2023-11-10 02:36:01,882 - root - INFO - Epoch: 2/2
	  Time:       0.060 sec
	  Train Loss: 65388.29622396
	  Test AUC:   70.51

2023-11-10 02:36:01,883 - root - INFO - Training time: 0.119
2023-11-10 02:36:01,883 - root - INFO - Finished training.
2023-11-10 02:36:02,367 - root - INFO - Start analyzing normal class: 5 / 7
2023-11-10 02:36:02,369 - root - INFO - Set seed to 42.
2023-11-10 02:36:02,369 - root - INFO - Computation device: cuda
2023-11-10 02:36:02,369 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:36:02,372 - root - INFO - Pretraining: True
2023-11-10 02:36:02,373 - root - INFO - 
---Pretraining Start---
2023-11-10 02:36:02,373 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:36:02,373 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:36:02,373 - root - INFO - Pretraining epochs: 4
2023-11-10 02:36:02,373 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:36:02,373 - root - INFO - Pretraining batch size: 20
2023-11-10 02:36:02,373 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:36:02,424 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:36:02,577 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:02,609 - root - INFO - Epoch: 1/4
	  Time:       0.183 sec
	  Train Loss: 76858078.66666667
	  Test Loss:  67784738.66666667
	  Test AUC:   41.88

2023-11-10 02:36:02,744 - root - INFO - Epoch: 2/4
	  Time:       0.134 sec
	  Train Loss: 75661426.66666667
	  Test Loss:  67781644.00000000
	  Test AUC:   41.88

2023-11-10 02:36:02,884 - root - INFO - Epoch: 3/4
	  Time:       0.140 sec
	  Train Loss: 76582066.66666667
	  Test Loss:  67783825.33333333
	  Test AUC:   41.88

2023-11-10 02:36:03,029 - root - INFO - Epoch: 4/4
	  Time:       0.143 sec
	  Train Loss: 75900834.66666667
	  Test Loss:  67786714.66666667
	  Test AUC:   41.88

2023-11-10 02:36:03,029 - root - INFO - Pretraining time: 0.604
2023-11-10 02:36:03,029 - root - INFO - Finished pretraining.
2023-11-10 02:36:03,036 - root - INFO - Testing autoencoder...
2023-11-10 02:36:03,060 - root - INFO - Test set Loss: 67786714.66666667
2023-11-10 02:36:03,061 - root - INFO - Autoencoder testing time: 0.025
2023-11-10 02:36:03,061 - root - INFO - Finished testing autoencoder.
2023-11-10 02:36:03,065 - root - INFO - 
---Training Start---
2023-11-10 02:36:03,065 - root - INFO - Training optimizer: adam
2023-11-10 02:36:03,065 - root - INFO - Training learning rate: 0.001
2023-11-10 02:36:03,065 - root - INFO - Training epochs: 2
2023-11-10 02:36:03,065 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:36:03,065 - root - INFO - Training batch size: 20
2023-11-10 02:36:03,065 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:36:03,068 - root - INFO - Initializing center c...
2023-11-10 02:36:03,075 - root - INFO - Center c initialized.
2023-11-10 02:36:03,075 - root - INFO - Starting training...
2023-11-10 02:36:03,112 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:03,113 - root - INFO - Epoch: 1/2
	  Time:       0.037 sec
	  Train Loss: 56887.59244792
	  Test AUC:   48.75

2023-11-10 02:36:03,150 - root - INFO - Epoch: 2/2
	  Time:       0.037 sec
	  Train Loss: 57503.01041667
	  Test AUC:   52.19

2023-11-10 02:36:03,150 - root - INFO - Training time: 0.074
2023-11-10 02:36:03,150 - root - INFO - Finished training.
2023-11-10 02:36:03,426 - root - INFO - Start analyzing normal class: 6 / 7
2023-11-10 02:36:03,428 - root - INFO - Set seed to 42.
2023-11-10 02:36:03,428 - root - INFO - Computation device: cuda
2023-11-10 02:36:03,428 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:36:03,432 - root - INFO - Pretraining: True
2023-11-10 02:36:03,432 - root - INFO - 
---Pretraining Start---
2023-11-10 02:36:03,432 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:36:03,432 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:36:03,432 - root - INFO - Pretraining epochs: 4
2023-11-10 02:36:03,432 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:36:03,432 - root - INFO - Pretraining batch size: 20
2023-11-10 02:36:03,432 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:36:03,483 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:36:03,600 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:03,630 - root - INFO - Epoch: 1/4
	  Time:       0.146 sec
	  Train Loss: 59672800.00000000
	  Test Loss:  67785720.00000000
	  Test AUC:   73.49

2023-11-10 02:36:03,769 - root - INFO - Epoch: 2/4
	  Time:       0.138 sec
	  Train Loss: 60177344.00000000
	  Test Loss:  67782522.66666667
	  Test AUC:   73.49

2023-11-10 02:36:03,914 - root - INFO - Epoch: 3/4
	  Time:       0.144 sec
	  Train Loss: 59830380.00000000
	  Test Loss:  67784326.66666667
	  Test AUC:   73.49

2023-11-10 02:36:04,071 - root - INFO - Epoch: 4/4
	  Time:       0.156 sec
	  Train Loss: 59237396.00000000
	  Test Loss:  67786410.66666667
	  Test AUC:   73.49

2023-11-10 02:36:04,071 - root - INFO - Pretraining time: 0.588
2023-11-10 02:36:04,071 - root - INFO - Finished pretraining.
2023-11-10 02:36:04,078 - root - INFO - Testing autoencoder...
2023-11-10 02:36:04,103 - root - INFO - Test set Loss: 67786410.66666667
2023-11-10 02:36:04,103 - root - INFO - Autoencoder testing time: 0.025
2023-11-10 02:36:04,103 - root - INFO - Finished testing autoencoder.
2023-11-10 02:36:04,108 - root - INFO - 
---Training Start---
2023-11-10 02:36:04,108 - root - INFO - Training optimizer: adam
2023-11-10 02:36:04,108 - root - INFO - Training learning rate: 0.001
2023-11-10 02:36:04,108 - root - INFO - Training epochs: 2
2023-11-10 02:36:04,108 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:36:04,108 - root - INFO - Training batch size: 20
2023-11-10 02:36:04,108 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:36:04,111 - root - INFO - Initializing center c...
2023-11-10 02:36:04,118 - root - INFO - Center c initialized.
2023-11-10 02:36:04,118 - root - INFO - Starting training...
2023-11-10 02:36:04,158 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:04,158 - root - INFO - Epoch: 1/2
	  Time:       0.040 sec
	  Train Loss: 49746.63151042
	  Test AUC:   32.56

2023-11-10 02:36:04,196 - root - INFO - Epoch: 2/2
	  Time:       0.037 sec
	  Train Loss: 48086.73437500
	  Test AUC:   33.49

2023-11-10 02:36:04,196 - root - INFO - Training time: 0.078
2023-11-10 02:36:04,196 - root - INFO - Finished training.
2023-11-10 02:36:12,314 - root - INFO - 
---Filtering Start---
2023-11-10 02:36:12,314 - root - INFO - Log file is ./DeepSVDD/log/log1699601772.314359.txt.
2023-11-10 02:36:12,314 - root - INFO - GPU is available.
2023-11-10 02:36:12,317 - root - INFO - Start analyzing normal class: 0 / 7
2023-11-10 02:36:12,320 - root - INFO - Set seed to 42.
2023-11-10 02:36:12,320 - root - INFO - Computation device: cuda
2023-11-10 02:36:12,320 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:36:12,325 - root - INFO - Pretraining: True
2023-11-10 02:36:12,325 - root - INFO - 
---Pretraining Start---
2023-11-10 02:36:12,325 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:36:12,325 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:36:12,325 - root - INFO - Pretraining epochs: 4
2023-11-10 02:36:12,325 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:36:12,325 - root - INFO - Pretraining batch size: 20
2023-11-10 02:36:12,325 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:36:12,383 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:36:12,500 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:12,531 - root - INFO - Epoch: 1/4
	  Time:       0.146 sec
	  Train Loss: 55245613.33333334
	  Test Loss:  68092137.33333333
	  Test AUC:   67.60

2023-11-10 02:36:12,686 - root - INFO - Epoch: 2/4
	  Time:       0.153 sec
	  Train Loss: 55907193.33333334
	  Test Loss:  68087489.33333333
	  Test AUC:   67.60

2023-11-10 02:36:12,831 - root - INFO - Epoch: 3/4
	  Time:       0.144 sec
	  Train Loss: 55810280.00000000
	  Test Loss:  68088497.33333333
	  Test AUC:   67.60

2023-11-10 02:36:12,979 - root - INFO - Epoch: 4/4
	  Time:       0.147 sec
	  Train Loss: 55207937.33333334
	  Test Loss:  68091525.33333333
	  Test AUC:   67.60

2023-11-10 02:36:12,979 - root - INFO - Pretraining time: 0.595
2023-11-10 02:36:12,979 - root - INFO - Finished pretraining.
2023-11-10 02:36:12,985 - root - INFO - Testing autoencoder...
2023-11-10 02:36:13,010 - root - INFO - Test set Loss: 68091525.33333333
2023-11-10 02:36:13,010 - root - INFO - Autoencoder testing time: 0.024
2023-11-10 02:36:13,010 - root - INFO - Finished testing autoencoder.
2023-11-10 02:36:13,015 - root - INFO - 
---Training Start---
2023-11-10 02:36:13,015 - root - INFO - Training optimizer: adam
2023-11-10 02:36:13,015 - root - INFO - Training learning rate: 0.001
2023-11-10 02:36:13,015 - root - INFO - Training epochs: 2
2023-11-10 02:36:13,015 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:36:13,015 - root - INFO - Training batch size: 20
2023-11-10 02:36:13,015 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:36:13,017 - root - INFO - Initializing center c...
2023-11-10 02:36:13,024 - root - INFO - Center c initialized.
2023-11-10 02:36:13,024 - root - INFO - Starting training...
2023-11-10 02:36:13,065 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:13,065 - root - INFO - Epoch: 1/2
	  Time:       0.041 sec
	  Train Loss: 45401.40494792
	  Test AUC:   31.01

2023-11-10 02:36:13,104 - root - INFO - Epoch: 2/2
	  Time:       0.039 sec
	  Train Loss: 44125.34895833
	  Test AUC:   34.49

2023-11-10 02:36:13,105 - root - INFO - Training time: 0.080
2023-11-10 02:36:13,105 - root - INFO - Finished training.
2023-11-10 02:36:13,381 - root - INFO - Start analyzing normal class: 1 / 7
2023-11-10 02:36:13,384 - root - INFO - Set seed to 42.
2023-11-10 02:36:13,384 - root - INFO - Computation device: cuda
2023-11-10 02:36:13,384 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:36:13,388 - root - INFO - Pretraining: True
2023-11-10 02:36:13,388 - root - INFO - 
---Pretraining Start---
2023-11-10 02:36:13,388 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:36:13,388 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:36:13,388 - root - INFO - Pretraining epochs: 4
2023-11-10 02:36:13,388 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:36:13,388 - root - INFO - Pretraining batch size: 20
2023-11-10 02:36:13,388 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:36:13,556 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:36:13,724 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:13,753 - root - INFO - Epoch: 1/4
	  Time:       0.196 sec
	  Train Loss: 69297452.00000000
	  Test Loss:  68095924.00000000
	  Test AUC:   55.56

2023-11-10 02:36:13,902 - root - INFO - Epoch: 2/4
	  Time:       0.147 sec
	  Train Loss: 74050133.33333333
	  Test Loss:  68089334.66666667
	  Test AUC:   55.56

2023-11-10 02:36:14,047 - root - INFO - Epoch: 3/4
	  Time:       0.144 sec
	  Train Loss: 74135746.66666667
	  Test Loss:  68089953.33333333
	  Test AUC:   55.56

2023-11-10 02:36:14,199 - root - INFO - Epoch: 4/4
	  Time:       0.151 sec
	  Train Loss: 72399806.66666667
	  Test Loss:  68091761.33333333
	  Test AUC:   55.56

2023-11-10 02:36:14,199 - root - INFO - Pretraining time: 0.643
2023-11-10 02:36:14,199 - root - INFO - Finished pretraining.
2023-11-10 02:36:14,206 - root - INFO - Testing autoencoder...
2023-11-10 02:36:14,232 - root - INFO - Test set Loss: 68091761.33333333
2023-11-10 02:36:14,232 - root - INFO - Autoencoder testing time: 0.026
2023-11-10 02:36:14,232 - root - INFO - Finished testing autoencoder.
2023-11-10 02:36:14,237 - root - INFO - 
---Training Start---
2023-11-10 02:36:14,237 - root - INFO - Training optimizer: adam
2023-11-10 02:36:14,237 - root - INFO - Training learning rate: 0.001
2023-11-10 02:36:14,237 - root - INFO - Training epochs: 2
2023-11-10 02:36:14,237 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:36:14,237 - root - INFO - Training batch size: 20
2023-11-10 02:36:14,237 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:36:14,240 - root - INFO - Initializing center c...
2023-11-10 02:36:14,247 - root - INFO - Center c initialized.
2023-11-10 02:36:14,248 - root - INFO - Starting training...
2023-11-10 02:36:14,284 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:14,284 - root - INFO - Epoch: 1/2
	  Time:       0.036 sec
	  Train Loss: 58124.41276042
	  Test AUC:   40.74

2023-11-10 02:36:14,319 - root - INFO - Epoch: 2/2
	  Time:       0.035 sec
	  Train Loss: 56672.15234375
	  Test AUC:   41.60

2023-11-10 02:36:14,319 - root - INFO - Training time: 0.072
2023-11-10 02:36:14,319 - root - INFO - Finished training.
2023-11-10 02:36:14,600 - root - INFO - Start analyzing normal class: 2 / 7
2023-11-10 02:36:14,602 - root - INFO - Set seed to 42.
2023-11-10 02:36:14,602 - root - INFO - Computation device: cuda
2023-11-10 02:36:14,602 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:36:14,606 - root - INFO - Pretraining: True
2023-11-10 02:36:14,606 - root - INFO - 
---Pretraining Start---
2023-11-10 02:36:14,606 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:36:14,606 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:36:14,606 - root - INFO - Pretraining epochs: 4
2023-11-10 02:36:14,606 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:36:14,606 - root - INFO - Pretraining batch size: 20
2023-11-10 02:36:14,606 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:36:14,659 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:36:14,822 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:14,854 - root - INFO - Epoch: 1/4
	  Time:       0.194 sec
	  Train Loss: 61933652.00000000
	  Test Loss:  68095404.00000000
	  Test AUC:   48.89

2023-11-10 02:36:15,008 - root - INFO - Epoch: 2/4
	  Time:       0.153 sec
	  Train Loss: 63607777.33333334
	  Test Loss:  68090384.00000000
	  Test AUC:   48.89

2023-11-10 02:36:15,163 - root - INFO - Epoch: 3/4
	  Time:       0.153 sec
	  Train Loss: 62126024.00000000
	  Test Loss:  68090145.33333333
	  Test AUC:   48.89

2023-11-10 02:36:15,319 - root - INFO - Epoch: 4/4
	  Time:       0.155 sec
	  Train Loss: 62912888.00000000
	  Test Loss:  68091564.00000000
	  Test AUC:   48.89

2023-11-10 02:36:15,319 - root - INFO - Pretraining time: 0.660
2023-11-10 02:36:15,319 - root - INFO - Finished pretraining.
2023-11-10 02:36:15,326 - root - INFO - Testing autoencoder...
2023-11-10 02:36:15,354 - root - INFO - Test set Loss: 68091564.00000000
2023-11-10 02:36:15,354 - root - INFO - Autoencoder testing time: 0.028
2023-11-10 02:36:15,354 - root - INFO - Finished testing autoencoder.
2023-11-10 02:36:15,360 - root - INFO - 
---Training Start---
2023-11-10 02:36:15,360 - root - INFO - Training optimizer: adam
2023-11-10 02:36:15,360 - root - INFO - Training learning rate: 0.001
2023-11-10 02:36:15,360 - root - INFO - Training epochs: 2
2023-11-10 02:36:15,360 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:36:15,360 - root - INFO - Training batch size: 20
2023-11-10 02:36:15,360 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:36:15,365 - root - INFO - Initializing center c...
2023-11-10 02:36:15,374 - root - INFO - Center c initialized.
2023-11-10 02:36:15,374 - root - INFO - Starting training...
2023-11-10 02:36:15,417 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:15,417 - root - INFO - Epoch: 1/2
	  Time:       0.043 sec
	  Train Loss: 52183.01562500
	  Test AUC:   44.44

2023-11-10 02:36:15,457 - root - INFO - Epoch: 2/2
	  Time:       0.039 sec
	  Train Loss: 51359.36718750
	  Test AUC:   48.89

2023-11-10 02:36:15,457 - root - INFO - Training time: 0.083
2023-11-10 02:36:15,457 - root - INFO - Finished training.
2023-11-10 02:36:15,734 - root - INFO - Start analyzing normal class: 3 / 7
2023-11-10 02:36:15,736 - root - INFO - Set seed to 42.
2023-11-10 02:36:15,736 - root - INFO - Computation device: cuda
2023-11-10 02:36:15,736 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:36:15,740 - root - INFO - Pretraining: True
2023-11-10 02:36:15,741 - root - INFO - 
---Pretraining Start---
2023-11-10 02:36:15,741 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:36:15,741 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:36:15,741 - root - INFO - Pretraining epochs: 4
2023-11-10 02:36:15,741 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:36:15,741 - root - INFO - Pretraining batch size: 20
2023-11-10 02:36:15,741 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:36:15,804 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:36:15,933 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:15,965 - root - INFO - Epoch: 1/4
	  Time:       0.160 sec
	  Train Loss: 54948305.33333334
	  Test Loss:  68093700.00000000
	  Test AUC:   50.57

2023-11-10 02:36:16,123 - root - INFO - Epoch: 2/4
	  Time:       0.156 sec
	  Train Loss: 55879213.33333334
	  Test Loss:  68088102.66666667
	  Test AUC:   50.57

2023-11-10 02:36:16,272 - root - INFO - Epoch: 3/4
	  Time:       0.147 sec
	  Train Loss: 54736340.00000000
	  Test Loss:  68088834.66666667
	  Test AUC:   50.57

2023-11-10 02:36:16,429 - root - INFO - Epoch: 4/4
	  Time:       0.156 sec
	  Train Loss: 55156957.33333334
	  Test Loss:  68090626.66666667
	  Test AUC:   50.57

2023-11-10 02:36:16,429 - root - INFO - Pretraining time: 0.625
2023-11-10 02:36:16,429 - root - INFO - Finished pretraining.
2023-11-10 02:36:16,436 - root - INFO - Testing autoencoder...
2023-11-10 02:36:16,463 - root - INFO - Test set Loss: 68090626.66666667
2023-11-10 02:36:16,463 - root - INFO - Autoencoder testing time: 0.027
2023-11-10 02:36:16,463 - root - INFO - Finished testing autoencoder.
2023-11-10 02:36:16,469 - root - INFO - 
---Training Start---
2023-11-10 02:36:16,469 - root - INFO - Training optimizer: adam
2023-11-10 02:36:16,469 - root - INFO - Training learning rate: 0.001
2023-11-10 02:36:16,469 - root - INFO - Training epochs: 2
2023-11-10 02:36:16,469 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:36:16,469 - root - INFO - Training batch size: 20
2023-11-10 02:36:16,469 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:36:16,473 - root - INFO - Initializing center c...
2023-11-10 02:36:16,481 - root - INFO - Center c initialized.
2023-11-10 02:36:16,481 - root - INFO - Starting training...
2023-11-10 02:36:16,522 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:16,523 - root - INFO - Epoch: 1/2
	  Time:       0.041 sec
	  Train Loss: 44590.84765625
	  Test AUC:   52.84

2023-11-10 02:36:16,563 - root - INFO - Epoch: 2/2
	  Time:       0.041 sec
	  Train Loss: 44157.47526042
	  Test AUC:   59.66

2023-11-10 02:36:16,564 - root - INFO - Training time: 0.083
2023-11-10 02:36:16,564 - root - INFO - Finished training.
2023-11-10 02:36:16,827 - root - INFO - Start analyzing normal class: 4 / 7
2023-11-10 02:36:16,830 - root - INFO - Set seed to 42.
2023-11-10 02:36:16,830 - root - INFO - Computation device: cuda
2023-11-10 02:36:16,830 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:36:16,835 - root - INFO - Pretraining: True
2023-11-10 02:36:16,835 - root - INFO - 
---Pretraining Start---
2023-11-10 02:36:16,835 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:36:16,835 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:36:16,835 - root - INFO - Pretraining epochs: 4
2023-11-10 02:36:16,835 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:36:16,835 - root - INFO - Pretraining batch size: 20
2023-11-10 02:36:16,835 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:36:16,899 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:36:17,183 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:17,214 - root - INFO - Epoch: 1/4
	  Time:       0.314 sec
	  Train Loss: 82091214.66666667
	  Test Loss:  68089582.66666667
	  Test AUC:   27.78

2023-11-10 02:36:17,485 - root - INFO - Epoch: 2/4
	  Time:       0.269 sec
	  Train Loss: 84976986.66666667
	  Test Loss:  68092242.66666667
	  Test AUC:   27.78

2023-11-10 02:36:17,755 - root - INFO - Epoch: 3/4
	  Time:       0.268 sec
	  Train Loss: 85370906.66666667
	  Test Loss:  68099822.66666667
	  Test AUC:   27.78

2023-11-10 02:36:18,042 - root - INFO - Epoch: 4/4
	  Time:       0.286 sec
	  Train Loss: 83826904.00000000
	  Test Loss:  68105909.33333333
	  Test AUC:   27.78

2023-11-10 02:36:18,042 - root - INFO - Pretraining time: 1.142
2023-11-10 02:36:18,042 - root - INFO - Finished pretraining.
2023-11-10 02:36:18,049 - root - INFO - Testing autoencoder...
2023-11-10 02:36:18,075 - root - INFO - Test set Loss: 68105909.33333333
2023-11-10 02:36:18,075 - root - INFO - Autoencoder testing time: 0.026
2023-11-10 02:36:18,075 - root - INFO - Finished testing autoencoder.
2023-11-10 02:36:18,080 - root - INFO - 
---Training Start---
2023-11-10 02:36:18,081 - root - INFO - Training optimizer: adam
2023-11-10 02:36:18,081 - root - INFO - Training learning rate: 0.001
2023-11-10 02:36:18,081 - root - INFO - Training epochs: 2
2023-11-10 02:36:18,081 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:36:18,081 - root - INFO - Training batch size: 20
2023-11-10 02:36:18,081 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:36:18,084 - root - INFO - Initializing center c...
2023-11-10 02:36:18,097 - root - INFO - Center c initialized.
2023-11-10 02:36:18,098 - root - INFO - Starting training...
2023-11-10 02:36:18,163 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:18,163 - root - INFO - Epoch: 1/2
	  Time:       0.065 sec
	  Train Loss: 67493.29947917
	  Test AUC:   67.36

2023-11-10 02:36:18,228 - root - INFO - Epoch: 2/2
	  Time:       0.065 sec
	  Train Loss: 65115.16080729
	  Test AUC:   68.75

2023-11-10 02:36:18,228 - root - INFO - Training time: 0.131
2023-11-10 02:36:18,228 - root - INFO - Finished training.
2023-11-10 02:36:18,734 - root - INFO - Start analyzing normal class: 5 / 7
2023-11-10 02:36:18,737 - root - INFO - Set seed to 42.
2023-11-10 02:36:18,737 - root - INFO - Computation device: cuda
2023-11-10 02:36:18,737 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:36:18,741 - root - INFO - Pretraining: True
2023-11-10 02:36:18,741 - root - INFO - 
---Pretraining Start---
2023-11-10 02:36:18,741 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:36:18,741 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:36:18,741 - root - INFO - Pretraining epochs: 4
2023-11-10 02:36:18,741 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:36:18,741 - root - INFO - Pretraining batch size: 20
2023-11-10 02:36:18,741 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:36:18,804 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:36:19,039 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:19,071 - root - INFO - Epoch: 1/4
	  Time:       0.265 sec
	  Train Loss: 73186696.00000000
	  Test Loss:  68091798.66666667
	  Test AUC:   39.38

2023-11-10 02:36:19,229 - root - INFO - Epoch: 2/4
	  Time:       0.157 sec
	  Train Loss: 73278128.00000000
	  Test Loss:  68086697.33333333
	  Test AUC:   39.38

2023-11-10 02:36:19,391 - root - INFO - Epoch: 3/4
	  Time:       0.160 sec
	  Train Loss: 75555553.33333333
	  Test Loss:  68088664.00000000
	  Test AUC:   39.38

2023-11-10 02:36:19,547 - root - INFO - Epoch: 4/4
	  Time:       0.155 sec
	  Train Loss: 74003458.66666667
	  Test Loss:  68090637.33333333
	  Test AUC:   39.38

2023-11-10 02:36:19,547 - root - INFO - Pretraining time: 0.742
2023-11-10 02:36:19,547 - root - INFO - Finished pretraining.
2023-11-10 02:36:19,554 - root - INFO - Testing autoencoder...
2023-11-10 02:36:19,581 - root - INFO - Test set Loss: 68090637.33333333
2023-11-10 02:36:19,581 - root - INFO - Autoencoder testing time: 0.027
2023-11-10 02:36:19,581 - root - INFO - Finished testing autoencoder.
2023-11-10 02:36:19,586 - root - INFO - 
---Training Start---
2023-11-10 02:36:19,586 - root - INFO - Training optimizer: adam
2023-11-10 02:36:19,586 - root - INFO - Training learning rate: 0.001
2023-11-10 02:36:19,586 - root - INFO - Training epochs: 2
2023-11-10 02:36:19,586 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:36:19,586 - root - INFO - Training batch size: 20
2023-11-10 02:36:19,586 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:36:19,589 - root - INFO - Initializing center c...
2023-11-10 02:36:19,597 - root - INFO - Center c initialized.
2023-11-10 02:36:19,597 - root - INFO - Starting training...
2023-11-10 02:36:19,635 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:19,635 - root - INFO - Epoch: 1/2
	  Time:       0.038 sec
	  Train Loss: 57111.51953125
	  Test AUC:   57.50

2023-11-10 02:36:19,670 - root - INFO - Epoch: 2/2
	  Time:       0.035 sec
	  Train Loss: 56714.34635417
	  Test AUC:   62.19

2023-11-10 02:36:19,670 - root - INFO - Training time: 0.073
2023-11-10 02:36:19,670 - root - INFO - Finished training.
2023-11-10 02:36:19,934 - root - INFO - Start analyzing normal class: 6 / 7
2023-11-10 02:36:19,935 - root - INFO - Set seed to 42.
2023-11-10 02:36:19,935 - root - INFO - Computation device: cuda
2023-11-10 02:36:19,935 - root - INFO - Number of dataloader workers: 0
2023-11-10 02:36:19,939 - root - INFO - Pretraining: True
2023-11-10 02:36:19,939 - root - INFO - 
---Pretraining Start---
2023-11-10 02:36:19,939 - root - INFO - Pretraining optimizer: adam
2023-11-10 02:36:19,939 - root - INFO - Pretraining learning rate: 0.001
2023-11-10 02:36:19,939 - root - INFO - Pretraining epochs: 4
2023-11-10 02:36:19,939 - root - INFO - Pretraining learning rate scheduler milestones: [0]
2023-11-10 02:36:19,939 - root - INFO - Pretraining batch size: 20
2023-11-10 02:36:19,939 - root - INFO - Pretraining weight decay: 1e-06
2023-11-10 02:36:19,991 - root - INFO - Starting pretraining on cuda...
2023-11-10 02:36:20,116 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:20,146 - root - INFO - Epoch: 1/4
	  Time:       0.154 sec
	  Train Loss: 59419540.00000000
	  Test Loss:  68091725.33333333
	  Test AUC:   78.14

2023-11-10 02:36:20,293 - root - INFO - Epoch: 2/4
	  Time:       0.145 sec
	  Train Loss: 61035641.33333334
	  Test Loss:  68087613.33333333
	  Test AUC:   78.14

2023-11-10 02:36:20,449 - root - INFO - Epoch: 3/4
	  Time:       0.155 sec
	  Train Loss: 60851158.66666666
	  Test Loss:  68088893.33333333
	  Test AUC:   78.14

2023-11-10 02:36:20,603 - root - INFO - Epoch: 4/4
	  Time:       0.152 sec
	  Train Loss: 60424816.00000000
	  Test Loss:  68091002.66666667
	  Test AUC:   78.14

2023-11-10 02:36:20,603 - root - INFO - Pretraining time: 0.612
2023-11-10 02:36:20,603 - root - INFO - Finished pretraining.
2023-11-10 02:36:20,609 - root - INFO - Testing autoencoder...
2023-11-10 02:36:20,636 - root - INFO - Test set Loss: 68091002.66666667
2023-11-10 02:36:20,636 - root - INFO - Autoencoder testing time: 0.026
2023-11-10 02:36:20,636 - root - INFO - Finished testing autoencoder.
2023-11-10 02:36:20,641 - root - INFO - 
---Training Start---
2023-11-10 02:36:20,641 - root - INFO - Training optimizer: adam
2023-11-10 02:36:20,641 - root - INFO - Training learning rate: 0.001
2023-11-10 02:36:20,641 - root - INFO - Training epochs: 2
2023-11-10 02:36:20,641 - root - INFO - Training learning rate scheduler milestones: [0]
2023-11-10 02:36:20,641 - root - INFO - Training batch size: 20
2023-11-10 02:36:20,641 - root - INFO - Training weight decay: 1e-06
2023-11-10 02:36:20,644 - root - INFO - Initializing center c...
2023-11-10 02:36:20,651 - root - INFO - Center c initialized.
2023-11-10 02:36:20,651 - root - INFO - Starting training...
2023-11-10 02:36:20,690 - root - INFO -   LR scheduler: new learning rate is 0.0001
2023-11-10 02:36:20,690 - root - INFO - Epoch: 1/2
	  Time:       0.038 sec
	  Train Loss: 49993.44661458
	  Test AUC:   22.79

2023-11-10 02:36:20,726 - root - INFO - Epoch: 2/2
	  Time:       0.036 sec
	  Train Loss: 49197.21875000
	  Test AUC:   25.12

2023-11-10 02:36:20,726 - root - INFO - Training time: 0.075
2023-11-10 02:36:20,726 - root - INFO - Finished training.
